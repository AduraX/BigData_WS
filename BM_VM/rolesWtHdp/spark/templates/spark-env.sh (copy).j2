#!/usr/bin/env bash

export SPARK_HOME={{proj_dir}}/spark
export SPARK_LOCAL_IP={{ spark_local_ip }} # the IP address Spark binds to on this node  {{ ansible_default_ipv4.address }}
export SPARK_LOCAL_DIRS={{ spark_local_dirs }} # | join(',')
export SPARK_WORKER_DIR={{ spark_worker_dir }}

export SPARK_MASTER_HOST={{ spark_master_host }}       # to bind the master to a different IP address or hostname
export SPARK_MASTER_PORT={{ spark_master_port }}       # to use non-default ports for the master
export SPARK_MASTER_WEBUI_PORT={{ spark_master_webui_port }}   # to use non-default ports for the master

#SPARK_WORKER_PORT={{ spark_worker_port }} # Start the Spark worker on a specific port (default: random).
#SPARK_WORKER_WEBUI_PORT={{ spark_worker_webui_port }}   # Port for the worker web UI (default: 8081).
export SPARK_WORKER_CORES={{ spark_worker_cores }}     # to set the number of cores to use on this machine
export SPARK_WORKER_MEMORY={{ spark_worker_memory }}    # to set how much total memory workers have to give executors (e.g. 1000m, 2g)
export SPARK_WORKER_OPTS="{{ spark_worker_opts }}"     # See: https://spark.apache.org/docs/latest/spark-standalone.html

export OPENBLAS_NUM_THREADS={{ spark_openblas_num_threads }}  # disable multi-threading of OpenBLAS (because Spark should be keeping CPUs busy)

#==== For PySpark use
# PYSPARK_PYTHON=python3
# PYSPARK_DRIVER_PYTHON='jupyter'
# PYSPARK_DRIVER_PYTHON_OPTS='notebook'
# PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

# HADOOP_HOME={{proj_dir}}/hadoop
# HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
# LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
# SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath) # hadoop libs needed for spark-*-without-hadoop distributions
